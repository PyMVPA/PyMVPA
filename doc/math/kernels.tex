\documentclass[a4paper,11pt]{article}
\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amsfonts}

\newcommand\R{{\mathbb R}}
\newcommand\x{{\mathbf x}}
\newcommand\X{{\mathbf X}}
\newcommand\K{{\mathbf K}}
\newcommand\J{{\mathbf J}}
\newcommand\LL{{\mathbf L}}
\newcommand\ELL{{\Ivec \ell}}
%%\newcommand\L{{\mathbf L}}
%%\DeclareMathOperator*{\argmax}{arg\,max}
%% \DeclareMathOperator*{\argmax}{argmax} %% (argmax wihtouth mid space)
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\var}{var}
\DeclareMathOperator*{\dm}{dm}
\newcommand{\Rvec}[1]{{\bf #1}}
\newcommand{\Ivec}[1]{\mbox{\boldmath $#1$}}

\title{Kernels}
\author{Emanuele Olivetti}

\begin{document}

\maketitle

\section{Introduction}
This document gives a detailed description of kernels implemented in
PyMVPA together with derivation of their gradients. Gradients are
useful when computing and maximizing the log marginal likelihood of a
Gaussian process approximating data.

The following notation is be used:
\begin{itemize}
\item $\x \in \R^D$ : a $D$-dimensional column vector, $\x =
  (x_1,\ldots,x_D)$.
\item $\X = (\x_1^{\top},\ldots,\x_N^{\top})$ : a $N \times D$
  matrix where each row is a $D$-dimensional vector. $\X$ is also
  called set of \emph{samples}. $\X_{* i}$ indicates the $i$-th
  column of $\X$ (column vector), and $\X_{j *}$ indicates the
  $j$-th row of $\X$ (row vector).
\item $k: \R^D \times \R^D \rightarrow \R$ : a covariance function.
\item $\K(\X,\X')$ : the matrix extension of $k$.\, i.e., $\K_{pq} =
  k(\x_p,\x'_q)$. If $\X$ is $N \times D$ and $\X'$ is $N' \times D$
  then $\K(\X,\X1)$ is $N \times N'$.
\item $\J_{n,m}$ : the $n \times m$ matrix of ones, i.e., a matrix
  where each element is 1. When $n = m$ it can be denoted as $\J_n$.
\item $\|\mathbf{z}\|_p$ : the $p$-norm of vector $\mathbf{z}$ defined
  as $\|\mathbf{z}\|_p = (\sum_{i=1}^D \|z_i\|^p)^{\frac{1}{p}}$.
  Euclidean norm is $p=2$, then $\|\mathbf{z}\|_2 =
  \sqrt{\mathbf{z}^{\top}\mathbf{z}} = \sqrt{\sum_{i=1}^D z_i^2}$.
\item $\|\mathbf{z},\mathbf{w}\|_p$ : the \emph{weighted} $p$-norm of
  vector $\mathbf{z}$ defined as $\|\mathbf{z},\mathbf{w}\|_p =
  (\sum_{i=1}^D w_i^p|z_i|^p)^{\frac{1}{p}}$.  Euclidean norm is
  $p=2$, then $\|\mathbf{z},\mathbf{w}\|_2 = \sqrt{\mathbf{z}^{\top}
    \mathbf{W}^{-1} \mathbf{z}} = \sqrt{\sum_{i=1}^D w_i^2 z_i^2}$,
  where $\mathbf{W} = diag(\mathbf{w})$.
\item $\dm(\X,\X')$ : the \emph{Euclidean distance matrix} between
  $\X$ and $\X'$ defined element by element as $\dm((\X,\X')_{pq} =
  \|\X_{p *} - \X'_{q *}\|_2 = \sqrt{\sum_{i=1}^D (\X_{p i} - \X'_{q
      i})^2}$. If $\X$ is $N \times D$ and $\X'$ is $N' \times D$ then
  $\dm(\X,\X1)$ is $N \times N'$.
\item $\dm(\X,\X',\mathbf{w})$ : the \emph{weighted} Euclidean
  distance matrix between $\X$ and $\X'$ defined element by element as
  $\dm(\X,\X',\mathbf{w})_{pq} = \|\X_{p i} - \X'_{q i}, \mathbf{w}\|_2
  = \sqrt{\sum_{i=1}^D w_i^2(\X_{p i} - \X'_{q i})^2}$ through the
  weight vector $\mathbf{w} \in \R^D$.
\item $\X \bullet \mathbf{Y}$ : the Hadamard (or Schur) matrix
  product, i.e. the entrywise product between matrices of the same
  size. Let $\mathbf{Z} = \X \bullet \mathbf{Y}$, then $z_{ij} =
  x_{ij} y_{ij}$.
\end{itemize}

\section{Constant kernel}
$$k(\x,\x') = \sigma_0^2$$
where $\sigma_0 \ge 0$ is the standard deviation of the Gaussian prior
probability $\mathcal{N}(0,\sigma_0^2)$ of the value of the constant.
$$\K(\X,\X') = \sigma_0^2 \J_{n,m}$$
$$\mathbf{\Theta} = \{\sigma_0\}$$
$$\frac{\partial k}{\partial \sigma_0}(\x,\x') = 2\sigma_0$$
$$\frac{\partial \K}{\partial \sigma_0} = 2\sigma_0 \J_{n,m}$$
$$A = \sigma_0^2$$
$$A \ge 0$$
$$k(\x,\x') = A$$
$$\K(\X,\X') = A \J_{n,m}$$
$$\frac{\partial k}{\partial A} = 1$$
$$\nabla \K_A = \frac{\partial \K}{\partial A} = \J_{n,m}$$

\section{Linear kernel}
Let $\Ivec{\Sigma}_p$ be the $D \times D$ covariance matrix of the Gaussian
prior probability $\mathcal{N}(\Ivec{0},\Ivec{\Sigma}_p)$ of the weights of
the Bayesian linear regression.
$$k(\x,\x') = \x^{\top} \Ivec{\Sigma}_p \x'$$
$$\K(\X,\X') = \X \Ivec{\Sigma}_p \X'^{\top}$$
In order to simplify formulas we assume $\Ivec{\Sigma}_p$ is diagonal, i.e.,
$\Ivec{\Sigma}_p = \Ivec{\sigma}^2_p I$ where $\Ivec{\sigma}^2_p =
\{{\sigma^2_p}_1,\ldots,{\sigma^2_p}_D\}$:
$$k(\x,\x') = \sum_{i=1}^D {\sigma^2_p}_i x_i x'_i$$
$$\mathbf{\Theta} = \{{\sigma_p}_1,\ldots,{\sigma_p}_D\}$$
$$\frac{\partial k}{\partial {\sigma_p}_i} = 2 {\sigma_p}_i x_i x'_i$$
$$A_i = {\sigma_p^2}_i$$
$$A_i \ge 0$$
$$\mathbf{\Theta}^* = \{ \mathbf{A} \}$$
$$\mathbf{A} = (A_1,\ldots,A_D)^{\top}$$
$$k(\x,\x') = \x^{\top} \mathbf{A}I \x'$$
$$\K(\X,\X') = \X \mathbf{A}I \X'^{\top}$$
$$\frac{\partial k}{\partial A_i} = x_i x'_i$$
$$\frac{\partial \K}{\partial A_i} = \X_{* i} {\X'_{* i}}^{\top}$$
$$\nabla_{\mathbf{A}} \K = ( \X_{* 1} {\X'_{* 1}}^{\top}, \ldots,
\X_{* D} {\X'_{* D}}^{\top})$$
As expected the gradient is independent of the hyperparameters values
and can be computed one at the beginning.

\section{Polynomial kernel}
$k(\x,\x') = (\x^{\top} \Ivec{\Sigma}_p \x')^p$

\section{Exponential kernel}
\subsection{Scalar Lengthscale $\ell$}
$$k(\x,\x') = \sigma_f^2 e^{-\frac{\|\x-\x'\|_2}{\ell}}$$
$$\ell \ge 0$$
$$\sigma_f \ge 0$$
$$\mathbf{\Theta} = \{ \sigma_f, \ell \}$$
$$\K(\X,\X') = \sigma_f^2 e^{-\frac{1}{\ell}\dm(\X,\X')}$$
$$A = \sigma_f^2$$
$$A \ge 0$$
$$B = -\frac{1}{\ell}$$
$$B \le 0$$
$$\mathbf{\Theta}^* = \{ A, B \}$$
$$k(\x,\x') = A e^{B\|\x-\x'\|_2}$$
$$\K(\X,\X') = A e^{B \dm(\X-\X')}$$
$$\frac{\partial k}{\partial A} = e^{B\|\x-\x'\|_2} = \frac{1}{A}k(\x,\x')$$
$$\frac{\partial \K}{\partial A} = e^{B \dm(\X,\X')} = \frac{1}{A} \K(\X,\X')$$
$$\frac{\partial k}{\partial B} = \|\x-\x'\|+2 A e^{B\|\x-\x'\|_2} =
\|\x-\x'\|_2 k(\x,\x')$$
$$\frac{\partial \K}{\partial B} = \dm(\X-\X') \bullet \K(\X,\X')$$
$$\nabla_{A,B} \K = (\frac{1}{A} \K(\X,\X'), \dm(\X-\X') \bullet
\K(\X,\X'))$$

Note that when $\K(\X,\X')$ is already computed, the gradient requires
just two element-by-element products to be computed, the second being
against a constant matrix independent of the hyperparameters. So a
mechanism that stores the precomputed kernel matrix related to a given
gradient will reduce greatly the number of computations
(memoization?).

\subsection{Vector of Lengthscales $\Ivec{\ell}$}
Given $\Ivec{\ell} = (\ell_1,\ldots,\ell_D)$, $\ell_i \ge 0$, $\LL =
diag(\ELL) = \Ivec{\ell}I$ and $\ELL^{-1} =
(1/\ell_1,\ldots,1/\ell_D)$
$$k(\x,\x') = \sigma_f^2 e^{-\|\x-\x',\ELL^{-1}\|_2}= \sigma_f^2
  e^{-\sqrt{\sum_{i=1}^D \left(\frac{x_i - x'_i}{\ell_i}\right)^2}}$$
$$K(\X,\X') = \sigma_f^2 e^{-\dm(\X,\X',\ELL^{-1})}$$
$$\mathbf{\Theta} = \{ \sigma_f, \ELL\}$$
$$A = \sigma_f^2$$
$$\mathbf{B} = \ELL^{-2}$$
$$\mathbf{\Theta}^* = \{ A, \mathbf{B}\}$$
$$k(\x,\x') = A e^{-\|\x-\x',\sqrt{\mathbf{B}}\|_2} = A e^{-\sqrt{\sum_{i=1}^D
    B_i(x_i - x'_i)^2}}$$  
$$K(\X,\X') = A e^{-\dm(\X,\X',\sqrt{\mathbf{B}})}$$
$$\frac{\partial k}{\partial A} = e^{-\sqrt{\sum_{i=1}^D B_i(x_i -
    x'_i)^2}} = \frac{k(\x,\x')}{A}$$
$$\frac{\partial \K}{\partial A} = e^{-\dm(\X,\X',\sqrt{\mathbf{B}})} =
\frac{\K(\X,\X')}{A}$$ 
$$\frac{\partial k}{\partial B_i} = A e^{-\|\x-\x',\sqrt{\mathbf{B}}\|_2}
\left( -\frac{1}{2} \|\x-\x',\sqrt{\mathbf{B}}\|_2^3 \right) (x_i - x'_i)^2 =
-\frac{1}{2} k(\x,\x') \|\x-\x',\sqrt{\mathbf{B}}\|_2^3 (x_i - x'_i)^2 $$
$$\frac{\partial \K}{\partial B_i} = -\frac{1}{2} K(\X,\X')
\bullet \dm(\X-\X',\sqrt{\mathbf{B}})^3 \bullet (\X_{*i}
(-\X'_{*i})^{\top})^2$$ 
$$\nabla_{A,\mathbf{B}} \K = \left(\frac{\K(\X,\X')}{A}, \left\{-\frac{1}{2}
  K(\X,\X') \bullet \dm(\X-\X',\sqrt{\mathbf{B}})^3 \bullet (\X_{*i}
  (-\X'_{*i})^{\top})^2 \right\}_{i=1 \ldots D} \right)$$

Note that $\frac{\partial \K}{\partial A}$ requires just the
multiplication of a constant by the kernel matrix $\K(\X,\X')$ (kernel
is usually already available). Instead $\frac{\partial \K}{\partial
  B_i}$ is a entrywise product of 3 matrices: $\K(\X,\X')$ (usually
already available), $\dm(\X-\X',\sqrt{\mathbf{B}})^3$ (which is,
except for the cube, part of the computation of $\K(\X,\X')$), and
$(\X_{*i} (-\X'_{*i})^{\top})^2$ which does not depend upon $A$ and
$\mathbf{B}$ so it can be computed once for all.

\section{Squared Exponential kernel}

\subsection{Scalar Lengthscale $\ell$}
$$k(\x,\x') = \sigma_f e^{-\frac{1}{2\ell^2} (\x-\x')^{\top}(\x-\x')}
  = \sigma_f e^{-\frac{1}{2\ell^2} \sum_{i=1}^D (x_i - x'_i)^2} =
  \sigma_f e^{-\frac{1}{2\ell^2} \|\x-\x'\|_2^2}$$ 
$$\K(\X,\X') = \sigma_f e^{-\frac{1}{2\ell^2} \dm(\X,\X')^2}$$
$$\ell \ge 0$$
$$\sigma_f \ge 0$$
$$\mathbf{\Theta} = \{ \sigma_f, \ell \}$$
$$A = \sigma_f^2$$
$$B = \ell^{-2}$$
$$\mathbf{\Theta}^* = \{ A, B\}$$
$$k(\x,\x') = A e^{B (\x-\x')^{\top}(\x-\x')}$$
$$\K(\X,\X') = A e^{B \dm(\X,\X')^2}$$
$$\frac{\partial k}{\partial A} = \frac{k(\x,\x')}{A}$$
$$\frac{\partial \K}{\partial A} = \frac{\K(\X,\X')}{A}$$
$$\frac{\partial k}{\partial B} = k(\x,\x') \|\x-\x'\|_2^2$$
$$\frac{\partial \K}{\partial B} = K(\X,\X') \bullet \dm(\X,\X')^2$$
$$\nabla_{A,B} \K = (\frac{1}{A} \K(\X,\X'), \dm(\X-\X')^2 \bullet
\K(\X,\X'))$$

Note that $\nabla_{A,B} \K$ is similar to that of the exponential
kernel so all comments over there applies here almost as well.


\subsection{Vector of Lengthscales $\ELL$}
Let $\mathbf{L} = diag(\ELL)$:
$$k(\x,\x') = \sigma_f^2 e^{-\frac{1}{2}(\x-\x')^{\top} \LL^{-2}
  (\x-\x')} = \sigma_f e^{-\frac{1}{2}\sum_{i=1}^D \frac{(x_i - x'_i)^2}{\ell_i^2}}$$
$$\K(\X,\X') = \sigma_f^2 e^{-\frac{1}{2} \dm(\X,\X',\ELL^{-1})^2}$$
$$\sigma_f \ge 0$$
$$\ell_i \ge 0$$
$$\mathbf{\Theta} = \{ \sigma_f, \ELL\}$$
$$A = \sigma_f$$
$$\mathbf{B} = -\frac{1}{2}\ELL^{-2} =
\left(-\frac{1}{2\ell_1^2},\ldots,-\frac{1}{2\ell_D^2} \right)$$
$$\mathbf{\Theta}^* = \{ A, \mathbf{B}\}$$
$$k(\x,\x') = A e^{(\x-\x')^{\top} diag(\mathbf{B}) (\x-\x')} = A
e^{\sum_{i=1}^D B_i (x_i - x'_i)^2}$$ 
$$\K(\X,\X') = A e^{\dm(\X,\X',\mathbf{B})}$$
$$\frac{\partial k}{\partial A} = \frac{k(\x,x')}{A}$$
$$\frac{\partial \K}{\partial A} = \frac{\K(\X,X')}{A}$$
$$\frac{\partial k}{\partial B} = k(\x,\x') (x_i -x'_i)^2$$
$$\frac{\partial \K}{\partial B} = \K(\X,\X') \bullet (\X_{*i}
(-\X'_{*i})^{\top})^2$$
$$\nabla_{A,\mathbf{B}} \K = \left(\frac{\K(\X,\X')}{A}, \left\{ K(\X,\X')
  \bullet (\X_{*i} (-\X'_{*i})^{\top})^2 \right\}_{i=1 \ldots D} \right)$$

Note that $\nabla_{A,\mathbf{B}} \K$ requires to compute $K(\X,\X')$
(which is usually already available), and its entrywise product with
$(\X_{*i} (-\X'_{*i})^{\top})^2$ which is independent of the value of
the hyperparameters and can be precomputed once for all. 

\section{$\gamma$-Exponential kernels}

\section{Mat\'ern kernels}

\section{Rational Quadratic kernels}

\end{document}
