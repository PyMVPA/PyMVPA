XXX Dataset might need to get labelweights parameter which could be
    used by some classifiers

Mapper                              Dataset (S1)
  |                                    \- appendtohistory(item)
  | \-ICAMapper          \             |
MaskMapper                \-------  MappedDataset (S1, Mapper)
   \ forward(ndarray)                  |
                                    MaskedDataset (S1, mask)
									   |
								  NiftiDataset

Splitter                               ErrorFunction
  |                                         |
NFoldSplitter(S2)                     CrossValidation
     \ call(dataset)



   Clf  (LinearSVM)
  train          \- weights 
  predict
      |
 BoostedClassifier    


 Clf  <- optimizeClassifier(optimizer, clf, parameter_to_optimize)



default combiner = average

 OptimizerAlgorithm
      |
 LineSearchOptimizer   GridSearch   FeatureStrip(SensitivityAnalyzer)
                                       after train return ErrorFunction, sensitivity, 


 Optimizer
      |
   ParameterOptimizer(optimizeralgorithm, parameter, classifier)
   BoostedOptimizer(splitter, optimizer)
   

# NOTES

Sensitivity -> RankList (N.argsort)

sens <- Combine
dataset <- selectImportantFeature

RFE(dataset, sensAnalyzer):
do
  sens <- sensAnalyzer(dataset)
  dataset <- selectImportantFeature(dataset, sens)
until the world collapses into


##############################################
 GET A GRIP ON: SensitivityAnalysers

# we need a new base class, here is the proposed name

DatasetMeasure / DatasetFunctor
# thinking again this might be a SensitivityAnalyser, but it does not provide
# a sens value per feature, but only a combined measure for all features
# together

# or even more generic: this is a class that provides a metric to quantify
# information in a dataset (not limited to classification). it should provide
# an interface that exposes this metric plus some interesting statistics
# (not sure what really generic candidates would be)
| \ provides vprop 'information' or similar
| \ __call__(dataset) # no further args
| |
| ClfCrossValidation(clf, splitter=NoneSplitter, errorfx=, combiner=MeanFx)
|  \__call__(dataset)
|   does what CrossValidation does now, but without the splitprocessor bloat
|   just computing a single performance value (e.g. mean generalization error)
|   however more exposed information is most likely required
|
|   NoneSplitter -- doesn't split and returns just original dataset
|   logic inside __call__ should not train a classifier if splitter
|   has only 1 dataset spitted out   (,dataset)
|
|   Assumption is such that splitter returns (train, predict,...)
|   (,dataset1) -> clf.predict(dataset1)
|   (dataset1, dataset2) -> clf.train(dataset1), clf.predict(dataset2)
|
|
|
SensitivityAnalyser
| |  |     \- sensitivity_map (1D array) <- __call__(dataset)
| |  |
| |  PerturbationSensitivityAnalyzer   LinearSVMSensitivityAnalyzer   NonLinearSVMAnalyzer
| |
| |   
| ClassifierBasedSensitivityAnalyzer(classifier)
|    |
|   CombinedSensitivityAnalyzer(splitter, sensanalyzer, combiner)
|         \-  __call__(dataset)
|
Searchlight(datasetmeasure, combinefx=None)
# Searchlight is a SensitivityAnalyser that runs another SensAna
#  with the 'DatasetMeasure'-interface
# on all possible spheres in the dataset
 \ [values] <- __call__(dataset)
 |   values = [None for i in dataset.Nfeatures]
 |   self->sizes = [0 for i in dataset.Nfeatures]
 |   foreach featireId do
 |       sphere = dataset.mapper.getNeighbor(featireId)
 |       values[featureId] = DatasetMeasure(dataset[sphere]) # d1: d2:
 |       self->sizes[featureId] = size(sphere)
 |   done
 |   return values
 | NB Exemplar code might look like dataset.mapper.reverse(N.array(searchlight(dataset)))
 |    to get the searchlightmap in original space
 \sensitivity_map
