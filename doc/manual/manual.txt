.. -*- mode: rst -*-
  ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ###
  #
  #   See COPYING file distributed along with the PyMVPA package for the
  #   copyright and license terms.
  #
  ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ###

PyMVPA Manual
=============

:Authors:
  Michael Hanke <michael.hanke@gmail.com>;
  Yaroslav O. Halchenko <debian@onerussian.com>
:Contact:  pkg-exppsy-pymvpa@lists.alioth.debian.org
:Homepage: http://pkg-exppsy.alioth.debian.org/pymvpa/
:Revision: unreleased

.. Please add yourself to the list of authors if you contribute something
   to this manual.

The latest version of this manual is avialable from the `PyMVPA project
website`_:

  * HTML: http://pkg-exppsy.alioth.debian.org/pymvpa/manual.html
  * PDF: http://pkg-exppsy.alioth.debian.org/pymvpa/files/manual.pdf

.. _PyMVPA project website: http://pkg-exppsy.alioth.debian.org/pymvpa/


.. meta::
   :description: The PyMVPA manual
   :keywords: python, machine learning, multivariate, neuroimaging

.. contents:: Table of Contents
.. sectnum::



Introduction
------------

PyMVPA is a Python_ module intended to ease pattern classification
analyses of large datasets. It provides high-level abstraction of typical
processing steps and a number of implementations of some popular algorithms.
While it is not limited to neuroimaging data it is eminently suited for such
datasets. PyMVPA is truely free software (in every respect) and additonally
requires nothing but free-software to run. Theoretically PyMVPA should run
on anything that can run a Python_ interpreter, although the proof is yet to
come.

PyMVPA stands for *Multivariate Pattern Analysis* in Python_.

.. _Python: http://www.python.org


What this Manual is NOT
~~~~~~~~~~~~~~~~~~~~~~~

This manual does not make an attempt to be a comprehensive introduction into
machine learning theory or pattern recognition techniques. There is a wealth
of high-quality text books about this field available. A very good example is:
`Pattern Recognition and Machine Learning`_ by `Christopher M. Bishop`_.

.. _Christopher M. Bishop: http://research.microsoft.com/~cmbishop/
.. _Pattern Recognition and Machine Learning: http://research.microsoft.com/~cmbishop/PRML

This manual does not describe every bit and piece of the PyMVPA package. For
more information, please have a look at the API documentation, which is a
comprehensive and up-to-date description of the whole package.
More examples and usage patterns extending the ones described here can be taken
from the examples shipped with the PyMVPA source distribution (`doc/examples/`)
or even the unit test battery, also part of the source distribution
(in the `tests/` directory).



A bit of History
~~~~~~~~~~~~~~~~

The roots of PyMVPA date back to early 2005. At that time it was a C++ library
(no Python_ yet) developed by Michael Hanke and Sebastian Kr√ºger with the
purpose to easily apply artificial neural networks to pattern recognition
problems.

During a visit to `Princeton University`_ in spring 2005 Michael Hanke
was introduced to the `MVPA toolbox`_ for `Matlab
<http://buchholz.hs-bremen.de/aes/aes_matlab.gif>`_. This toolbox was
and is mainly developed by `Greg Detre`_ and had several advantages
over a C++ library. Most importantly it was easier to use. While a
user of a C++ library is forced to write a significant amount of
front-end code, users of the MVPA toolbox could simply load their data
and start analyzing it. Besides that the MVPA toolbox offered a number
of algorithms that were not implmented in the C++ library.

.. _Princeton University: http://www.princeton.edu
.. _MVPA toolbox: http://www.csbmb.princeton.edu/mvpa/
.. _Greg Detre: http://www.gregdetre.co.uk

However, writing a Matlab toolbox implies some disadvantages that also
apply to the MVPA toolbox. While users in general benefit from the powers
of Matlab, they are at the same time bound to the goodwill of a commercial
company. That this is indeed a problem becomes obvious when one consideres the
time when the vendor of Matlab was not willing to support the Mac platform.
Therefore even if the MVPA toolbox is `GPL-licensed`_ it cannot fully benefit
from the enourmous advantages of the free-software development model
environment (free as in free speech, not only free beer).

.. _GPL-licensed: http://www.gnu.org/copyleft/gpl.html

Under this impression Michael thought that a successor of the C++ library
should remain truely free-software, remain fully object-oriented (in contrast
to the MVPA toolbox), but should be at least as easy to use and extensible
as the MVPA toolbox.

After evaluating some possibilities Michael decided that `Python`_ is the
most promissing candidate that was fully capable to fulfil the intended
development goal. Python is a very powerful language that magically combines
the possibility to write really fast code and a simplicity that allows to
learn the basic concepts within a few days.

One of the major advantages of Python is the availablity of a huge amount of
so called *modules*. Modules can include extensions written in a hard-core
language like C and therefore allow to incorporate high-performance code
without having to leave the Python environment. Additionally some Python
modules even provide links to other toolkits. For example `RPy`_ allows to use
the full functionality of R_ from inside Python. Even Matlab can be used via
some Python modules (see PyMatlab_ for an example).

.. _RPy: http://rpy.sourceforge.net/
.. _R: http://www.r-project.org
.. _PyMatlab: http://code.google.com/p/pymatlab/

After the decision for Python was made, Michael started development with a
simple k-Nearest-Neighbour classifier and a cross-validation class. Using
the mighty NumPy_ package made it easy to support data of any dimensionality.
Therefore PyMVPA can easily be used with 4d fMRI dataset, but equally well
with EEG data (3d) or even non-neuroimaging datasets.

By September 2007 PyMVPA included support for reading and writing datasets
from and to the `NIfTI format`_, kNN and Support Vector Machine classifiers,
as well as several analysis algorithms (e.g. searchlight and incremental
feature search).

.. _NIfTI format: http://nifti.nimh.nih.gov/

During another visit in Princeton in October 2007 Michael met with `Yaroslav
Halchenko`_ and `Per B. Sederberg`_. That incident and the following
discussions and hacking sessions of Michael and Yaroslav lead to a major
refactoring of the PyMVPA codebase, making it much more flexible/extensible,
faster and easier as it has ever been before.

.. _Yaroslav Halchenko: http://www.onerussian.com/
.. _Per B. Sederberg: http://www.princeton.edu/~persed/


Prerequisites
~~~~~~~~~~~~~

Like every other Python module PyMVPA requires at least a basic knowledge of
the Python language. However, if one has no prior experience with Python one
can benefit from the simplicity of the Python language and acquire this
knowledge within a few days by studying some of the many tutorials available
on the web.

.. links to good tutorials

As PyMVPA is about pattern recognition a basic understanding about machine
learning principles is necessary to correctly apply methods with PyMVPA to
ensure interpretablity of the results.


Dependencies
''''''''''''

The following software packages are required or PyMVPA will not work at all.

  Python_ 2.4 (or later)
	With some modifications PyMVPA could probably work with Python 2.3, but as
	it is quite old already and Python 2.4 is widely available there should be
	no need to do this.
  NumPy_
	PyMVPA makes extensive use of NumPy to store and handle data. There is no
	way around it.

.. _NumPy: http://numpy.scipy.org/


Strong Recommendations
''''''''''''''''''''''

While most parts of PyMVPA will work without any additonal software, some
functionality makes use of additonal software packages. It is strongly
recommend to install these packages as well.

  SciPy_: linear algebra, standard distributions
	Mainly code for statistical testing and the logistic regression classifier
	make use of SciPy_. However, in the long run SciPy might be used a lot
	more and could become a required dependency of PyMVPA.
  libsvm_: fast SVM classifier
	Only the C library is required and none of the Python bindings available
	on the upstream website. PyMVPA provides its own Python wrapper for libsvm
	which is a fork based on the one included in the libsvm package.
  PyNIfTI_: access to NIfTI files
	PyMVPA provides a convenient wrapper for datasets stored in the NIfTI
	format. If you don't need that, PyNIfTI is not necessary, but otherwise
	it makes it really easy to read from and write to NIfTI images.

.. _SciPy: http://www.scipy.org/
.. _libsvm: http://www.csie.ntu.edu.tw/~cjlin/libsvm/
.. _PyNIfTI: http://niftilib.sourceforge.net/pynifti/


Suggestions
''''''''''''

The following list of software is not required by PyMVPA, but it might make
life a lot easier and lead to more efficiency when using PyMVPA.

  IPython_: frontend
	If you want to use PyMVPA interactively it is strongly recommend to use
	IPython_. If you think: *"Oh no, not another one, I already have to learn
	about PyMVPA."* please invest a tiny bit of time to watch the `Five Minutes
	with IPython`_ screencasts at showmedo.com_, so at least you know what you
	are missing.
  FSL_: preprocessing and analysis of (f)MRI data
	PyMVPA provides some simple bindings to FSL output and filetypes (e.g. EV
	files and MELODIC output directories). This makes it fairly easy to e.g.
	use FSL's implementation of ICA for data reduction and proceed with
	analyzing the estimated ICs in PyMVPA.

.. _IPython: http://ipython.scipy.org
.. _Five Minutes with IPython: http://showmedo.com/videos/series?name=CnluURUTV
.. _showmedo.com: http://showmedo.com
.. _FSL: http://www.fmrib.ox.ac.uk/fsl/


Obtaining PyMVPA
~~~~~~~~~~~~~~~~

Binary packages
'''''''''''''''

Binary packages are not available yet, but will be when the first release of
PyMVPA has happened. And there will be a Debian package of course.


Building from Source
''''''''''''''''''''

Source code tarballs of PyMVPA releases are available from the `PyMVPA
project website`_. To get access to both the full PyMVPA history and the latest
development code the PyMVPA Git_ repository is publicly available. To view the
repository, please point your webbrowser to gitweb:
http://git.debian.org/?p=pkg-exppsy/pymvpa.git

To clone (aka checkout) the PyMVPA repository simply do:

::

  git clone git://git.debian.org/git/pkg-exppsy/pymvpa.git

After a short while you will have a `pymvpa` directory below your current
working directory, that contains the PyMVPA repository.

.. _Git: http://git.or.cz/


Installation
~~~~~~~~~~~~

(to be written)

.. Point to source and binary distribution. Preach idea of free software.
   Step by step guide to install it on difficult systems like Windows.

.. Don't forget to mention that the only reasonable way is to use this piece
   of software (like every other piece) is under Debian! Also mention that
   Ubuntu is no excuse ;-)


How to cite PyMVPA
~~~~~~~~~~~~~~~~~~

(to be written)

.. come up with something



Credits
~~~~~~~

(to be written)

.. Please add some notes when you think that you should give credits to someone
   that enables or motivates you to work on PyMVPA ;-)


Manual Conventions
~~~~~~~~~~~~~~~~~~

In all examples the NumPy package is assumed to be imported using the alias N
throughout this manual.

  >>> import numpy as N



Overview
--------

The PyMVPA package consists of three major parts: `Data handling`_,
Classifiers_ and Algorithms_ operating on datasets and classifiers.
In the following sections the basic concept of all three parts will be
described and examples using certain parts of the PyMVPA package will be
given. 

Data Handling
-------------

The foundation of PyMVPA's data handling is the Dataset_ class. Basically,
this class stores data samples, sample attributes and dataset attributes, where
sample attributes assign a value to each data sample and dataset attributes are
additional information or functionality that applies to the whole dataset.

.. _Dataset: api/mvpa.datasets.dataset.Dataset-class.html

Most likely the Dataset_ class will not be used directly, but through one
of the derived classes. However, it is perfectly possible to use it directly.
In the simplest case a dataset can be constructed by specifying some
data samples and the corresponding class labels.

  >>> from mvpa.datasets.dataset import Dataset
  >>> data = Dataset(samples=N.random.normal(size=(10,5)), labels=1)
  >>> data
  Dataset / float64 10 x 5, uniq: 1 labels, 10 chunks

The above example creates a dataset with 10 samples and 5 features each. The
values of all features stem from normally distributed random noise. The class
label '1' is assigned to all samples. Instead of a single scalar value `labels`
can also be a sequence with individual labels for each data sample. In this
case the length of this sequence has to match the number of samples.

Interestingly the dataset object tells us about 10 `chunks`. In PyMVPA chunks
are used to group subsets of data samples. However, if no grouping information
is provided all data samples are assumed to be in their own group, hence no
sample grouping is performed.

Both `labels` and `chunks` are so called *sample attributes*. All sample
attributes are stored in sequence-type containers consisting of one value per
sample. These containers can be accessed by properties with the same as the
attribute:

  >>> data.labels
  array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
  >>> data.chunks
  array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

The *data samples* itself are stored as a two-dimensional matrix where each
row vector is a `sample` and each column vector contains the values of a
`feature` across all `samples`. The Dataset_ class provides access to the
samples matrix via the `samples` property.

  >>> data.samples.shape
  (10,5)

The Dataset_ class itself can only deal with 2d sample matrices. However,
PyMVPA provides a very easy way to deal with data where each data sample is
more than a 1d vector: `Data Mapping`_


Data Mapping
~~~~~~~~~~~~
It was already mentioned that the Dataset_ class cannot deal with data samples
that are more than simple vectors. This could be a problem in cases where the
data has a higher dimensionality, e.g. functional brain-imaging data where
each data sample is typically a three-dimensional volume.

One approach to deal with this situation would be to concatenate the whole
volume into a 1d vector. While this would work in certain cases there is
definitely information lost. Especially for brain-imaging data one would most
likely want keep information about neighbourhood and distances between data
sample elements.

In PyMVPA this is done by mappers that transform data samples from their
original *dataspace* into the so-called *features space*. In the above
neuro-imaging example the *dataspace* is three-dimensional and the
*feature space* always refers to the 2d `samples x features` representation
that is required by the Dataset_ class. In the context of mappers the
dataspace is sometimes also refered to as *in-space* while the feature space
is labeled as *out-space*.

The task of a mapper besides transforming samples into 1d vectors is to retain
as much information of the dataspace as possible. Some mappers provide
information about dataspace metrics and feature neighbourhood, but all mappers
are able to do reverse mapping from feature space into the original dataspace.

Usually one does not have to deal with mappers directly. PyMVPA provides some
convenience subclasses of Dataset_ the automatically perform the necessary
mapping operations internally. 

For an introduction into to concept of a dataset with mapping capabilities
we can take a look at the MaskedDataset_ class. This dataset class works
almost exactly like the basic Dataset_ class, except that it provides some
additional method and is more flexible with respect to the format of the sample
data. A masked dataset can be created just like a normal dataset.

.. _MaskedDataset: api/mvpa.datasets.maskeddataset.MaskedDataset-class.html

  >>> from mvpa.datasets.maskeddataset import MaskedDataset
  >>> mdata = MaskedDataset(samples=N.random.normal(size=(5,2,3,4)),
  ...                       labels=[1,2,3,4,5])
  >>> mdata
  Dataset / float64 5 x 24 uniq: 5 labels 5 chunks

However, unlike Dataset_ the MaskedDataset_ class can deal with sample
data arrays with more than two dimensions. More precisely it handles arrays of
any dimensionality. The only assumption that is made is that the first axis
of a sample array separates the sample data points. In the above example we
therefore have 5 samples, where each sample is a 2x3x4 volume.

If we look at the self-description of the created dataset we can see, that it
doesn't tell us about 2x3x4 volumes, but simply 24 features. That is because
internally the sample array is automatically reshaped into the aforementioned
2d matrix representation of the Dataset_ class. However, the information about
the original dataspace is not lost, but kept inside the mapper used by
MaskedDataset_. Two useful methods of MaskedDataset_ make use of the mapper:
`mapForward()` and `mapReverse()`. The former can be used to transform
additional data from dataspace into the feature space and the latter performs
the same in the opposite direction.

  >>> mdata.mapForward(N.arange(24).reshape(2,3,4))
  (24,)
  >>> mdata.mapReverse(N.array([1]*mdata.nfeatures)).shape
  (2, 3, 4)

Especially reverse mapping can be very useful when visualizing classification
results and information maps on the original dataspace.

Another feature of mapped datasets is that valid mapping information is
maintained even when the feature space changes. When running some feature
selection algorithm (see Algorithms_) some features of the original features
set will be removed, but after feature selection one will most likely want
to know where the selected (or removed) features are in the original dataspace.
To make use of the neuro-imaging example again: The most convenient way to
access this kind of information would be a map of the selected features that
can be overlayed over some anatomy image. This is trivial with PyMVPA, because
the mapping is automatically updated upon feature selection.

  >>> mdata.mapReverse(N.arange(1,mdata.nfeatures+1))
  array([[[ 1,  2,  3,  4],
          [ 5,  6,  7,  8],
          [ 9, 10, 11, 12]],
         [[13, 14, 15, 16],
          [17, 18, 19, 20],
          [21, 22, 23, 24]]])
  >>> sdata = mdata.selectFeatures([2,7,9,10,16,18,20,21,23])
  >>> sdata
  Dataset / float64 5 x 9 uniq: 5 labels 5 chunks
  >>> sdata.mapReverse(N.arange(1,sdata.nfeatures+1))
  array([[[0, 0, 1, 0],
          [0, 0, 0, 2],
          [0, 3, 4, 0]],
         [[0, 0, 0, 0],
          [5, 0, 6, 0],
          [7, 8, 0, 9]]])

The above example selects nine features from the set of the 24 orginal
ones, by passing their ids to the `selectFeatures()` method. The method
returns a new dataset only containing the nine selected features. Both datasets
share the sample data (using a NumPy array view). Using `selectFeatures()`
is therefore both memory efficient and relatively fast. All other
information like class labels and chunks are maintained. By calling
`mapReverse()` on the new dataset one can see that the remaining nine features
are precisely mapped back onto their original locations in the data space.


Data Splitting
~~~~~~~~~~~~~~

In many cases some algorithm should not run on a complete dataset, but just
some parts of it. One well-known example is leave-one-out cross-validation,
where a dataset is typically split into a number of training and validation
datasets. A classifier is trained on the training set and it's generalization
performance is tested using the validation set.

It is important to strictly separate training and validation datasets as
otherwise no valid statement can be made whether a classifier really generated
an appropriate model of the training data. If this requirement is violated
the results are usually very good generalization performances. However, they
provide no relevant information as they are based on cheating or peeking and
do not describe signal similarities between training and validation datasets.

With the splitter classes, PyMVPA makes dataset splitting easy. All dataset
splitters in PyMVPA are implemented as Python generators, meaning that when
called with a dataset once, they return one dataset split per iteration and
an appropriate Exception when they are done. This is exactly the same behavior
as of e.g. the Python `xrange()` function.

To perform data splitting for the already mentioned cross-validation, PyMVPA
provides the NFoldSplitter_ class. It implements a method to generate
arbitrary N-M splits, where N is the number of different chunks in a dataset
and M is any non-negative integer smaller than N. Doing a leave-one-out split
of our example dataset looks like this:

.. _NFoldSplitter: api/mvpa.datasets.splitter.NFoldSplitter-class.html

  >>> from mvpa.datasets.splitter import NFoldSplitter
  >>> splitter = NFoldSplitter(cvtype=1)   # Do N-1
  >>> for wdata, vdata in splitter(data):
          # do something
          pass

where `wdata` is the *working dataset* and `vdata` is the *validation dataset*.
If we have a look a those datasets we can see that the splitter did what we
intended:

  >>> split = [ i for i in splitter(data)][0]
  >>> split
  (Dataset / float64 9 x 5 uniq: 1 labels 9 chunks,
   Dataset / float64 1 x 5 uniq: 1 labels 1 chunks)
  >>> split[0].uniquechunks
  array([1, 2, 3, 4, 5, 6, 7, 8, 9])
  >>> split[1].uniquechunks
  array([0])

In the first split, the working dataset contains nine chunks of the original
dataset and the validation set contains the remaining chunk.

The usage of the splitter, creating a splitter object and calling it with a
dataset, is a very common design pattern in the PyMVPA package. Like splitters
there are many more so called *processing objects*. These classes or objects
are instanciated by passing all relevant parameters to the constructor. 
Processing objects can then be called multiple times with different datasets
to perform their algorithm on the respective dataset. This design applies to
virtually every piece of PyMVPA that is described in the Algorithms_ section,
but also the many other parts.



Classifiers
-----------

PyMVPA includes a number of ready-to-use classifiers, which are described in
the following sections. All classifiers implement the same, very simple
interface. Each classifier object takes all relevant parameters as arguments
to it's constructor. Once instanciated, the object's `train()` method can be
called with some dataset. This trains the classifier using *all* samples
in the respective dataset.

The major task for a classifier is to make predictions. Predictions are made
by calling the classifier's `predict()` method with one or multiple data
samples. `predict()` operates on pure sample data and not datasets, as in
some cases the true label for a sample might be totally unknown.

This examples demonstrates the typical daily life of a classifier. 

  >>> from mvpa.clfs.knn import kNN
  >>> from mvpa.datasets.dataset import Dataset
  >>> training = Dataset(samples=N.array(N.arange(100),ndmin=2).T,
                         labels=[0] * 50 + [1] * 50)
  >>> rand100 = N.random.rand(10)*100
  >>> validation = Dataset(samples=N.array(rand100, ndmin=2).T,
                           labels=[ int(i>50) for i in rand100 ])
  >>> clf = kNN(k=10)
  >>> clf.train(training)
  >>> N.mean(clf.predict(training.samples) == training.labels)
  1.0
  >>> N.mean(clf.predict(validation.samples) == validation.labels)
  1.0

Two datasets with 100 and 10 samples each are generated. Both datasets only
have one feature and the associated label is 0 if the feature value is below
50 or 1 otherwise. The larger dataset contains all integers in the intervall
(0,100) and is used to train the classifier. The smaller is used as a
validation dataset, to check whether the classifier learned something that
generalizes well across samples not included in the training dataset. In this
case the validation dataset consists of 10 random floating point values in the
interval (0,100).

The classifier in this example is a k-Nearest-Neighbour_ classifier that makes
use of the 10 nearest neighbours of a data sample to make it's predictions
(k=10). One can see that after the training the classifier performs optimal on
the training dataset as well as on the validation data samples.

The choice of the classifier in the above example is more or less arbitrary.
Any classifier in PyMVPA can replace the choosen kNN object. This demonstrates
another useful feature of PyMVPA's classifiers. Due to the high-level
abstraction and the simple interface, almost all classifiers can be combined
with most algorithms in PyMVPA (please see the Algorithms_ section for
details). This makes it very easy to test different classifiers on some
dataset.


Stateful objects
~~~~~~~~~~~~~~~~

Before looking at the different classifiers in more detail, it is important to
mention another feature common to all of them. While their interface is simple,
classifiers are in no way limited to report nothing but predictions. All
classifiers implemenent an additional interface: the so called `State`.
This means basically, that a classifier object can be treated similar to a
Python dictionary. This dictionary contains additional classifier-specific
information.

To continue the last example, each classifier, or more precisely every objects
that implements the `State` interface can be asked to report existing
state-related information:

  >>> clf.states
  ['values', 'predictions']

By default all classifiers provide two state variables `values` and
`predictions`. The latter is simply the set of predictions that was returned
by the last call to the objects `predict()` method. The former is heavily
classifier-specific. By convention the `values` key provides access to the
raw values that a classifier prediction is based on. Depending on the
classifier this information might required significant ressources when stored.
Therefore all states can be disabled or enabled (`disableState()`,
`enableState()`) and their current status can be queried like this:

  >>> clf.isStateActive('predictions')
  True
  >>> clf.isStateActive('values')
  False
  >>> clf.enabledStates
  ['predictions']


Error Calculation
~~~~~~~~~~~~~~~~~

TransferError_

(to be written)

.. _TransferError: api/mvpa.clfs.transerror.TransferError-class.html


Boosted and Multi-class Classifiers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

(to be written)

.. Point to the special case of multi-class classification and how to deal with
   it. Finally describe features of all available classifiers.


k-Nearest-Neighbour
~~~~~~~~~~~~~~~~~~~

kNN_

(to be written)

.. _kNN: api/mvpa.clfs.knn.kNN-class.html


Support Vector Machines
~~~~~~~~~~~~~~~~~~~~~~~

LinearCSVMC_
LinearNuSVMC_
RbfNuSVMC_
RbfCSVMC_
SVMBase_

(to be written)

.. _LinearCSVMC: api/mvpa.clfs.svm.LinearCSVMC-class.html
.. _LinearNuSVMC: api/mvpa.clfs.svm.LinearNuSVMC-class.html
.. _RbfCSVMC: api/mvpa.clfs.svm.RbfCSVMC-class.html
.. _RbfNuSVMC: api/mvpa.clfs.svm.RbfNuSVMC-class.html
.. _SVMBase: api/mvpa.clfs.svm.SVMBase-class.html

Logistic Regression
~~~~~~~~~~~~~~~~~~~

PLF_ (why is this called PLF not PLR?)

(to be written)

.. _PLF: api/mvpa.clfs.plf.PLF-class.html


Algorithms
----------

PyMVPA provides a number of useful algorithms. The vast majority of them is
dedicated to feature selection. To increase the flexiblity of analyses PyMVPA
distinguished two different parts of a feature selection procedure. First,
the impact of each individual feature on a classification has to be determined.
The resulting map reflects the sensitivities of all features with respect to
a certain decision, therefore algorithms generating these maps are called
`Sensitivity Analyzers`_ in PyMVPA.

When the feature sensitivities are known they can be used as a criterion for
feature selection. However, possible selection strategies range from very
simply *Go with the 10% best features* to more complicated algorithms like
*Recursive feature selection* (RFE_). Because `Sensitivity Analyzers`_ and
selections strategies can be arbitrarily combined, PyMVPA offers a quite
flexible framework for feature selection.

Similar to dataset splitters all PyMVPA algorithms are implemented and
behave like *processing objects*. To recap, this means that they are
instanciated by passing all relevant arguments to the constructor. Once
created, they can be used multiple times by calling them with different
datasets.

.. Again general overview first. What is a `SensitivityAnalyzer`, what is the
   difference between a `FeatureSelection` and an `ElementSelector`.
   Finally more detailed note and references for each larger algorithm.


Sensitivity Analyzers
~~~~~~~~~~~~~~~~~~~~~

SensitivityAnalyzer_

(to be written)

.. _SensitivityAnalyzer: api/mvpa.algorithms.datameasure.SensitivityAnalyzer-class.html

.. Point to the difference of `DataMeasure` and `SensitivityAnalyzer`. The
   former computes some value given a dataset (might be scalar or something
   else), whereas that latter computes a 1d map (sensitivity -> features),
   where each value assigns a quantification of the amount of available
   information in that feature.

Noise Perturbation
''''''''''''''''''

PerturbationSensitivityAnalyzer_

(to be written)

.. _PerturbationSensitivityAnalyzer: api/mvpa.algorithms.perturbsensana.PerturbationSensitivityAnalyzer-class.html

ANOVA
'''''

OneWayAnova_

(to be written)

.. _OneWayAnova: api/mvpa.algorithms.anova.OneWayAnova-class.html


Linear SVM Weights
''''''''''''''''''

LinearSVMWeights_

(to be written)

.. _LinearSVMWeights: api/mvpa.algorithms.linsvmweights.LinearSVMWeights-class.html


Splitting Sensitivity Analyzer
''''''''''''''''''''''''''''''

SplittingSensitivityAnalyzer_

(to be written)

.. _SplittingSensitivityAnalyzer: api/mvpa.algorithms.splitsensana.SplittingSensitivityAnalyzer-class.html


Feature Selection Strategies
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Recursive Feature Elimination
'''''''''''''''''''''''''''''

RFE_

(to be written)

.. _RFE: api/mvpa.algorithms.rfe.RFE-class.html


Incremental Feature Search
''''''''''''''''''''''''''

IFS_

(to be written)

.. _IFS: api/mvpa.algorithms.ifs.IFS-class.html

.. What are the practical differences (besides speed) between RFE and IFS?


Classifier Cross-Validation
~~~~~~~~~~~~~~~~~~~~~~~~~~~

ClfCrossValidation_

(to be written)

.. _ClfCrossValidation: api/mvpa.algorithms.clfcrossval.ClfCrossValidation-class.html


Searchlight
~~~~~~~~~~~

Searchlight_

(to be written)

.. _Searchlight: api/mvpa.algorithms.searchlight.Searchlight-class.html

.. Mention the fact that it also is a special `SensitivityAnalyzer`



Statistical Testing
~~~~~~~~~~~~~~~~~~~

NullHypothesisTest_

(to be written)

.. _NullHypothesisTest: api/mvpa.algorithms.nullhyptest.NullHypothesisTest-class.html

.. Point to the problem of an unknown H0 distribution, which is a problem
   for a lot of statistical tests.


Progress Tracking
-----------------
.. some parts should migrate into developer reference I guess

There are 3 types of messages PyMVPA can produce:

 verbose_
   regular informative messages about generic actions being performed
 debug_
   messages about the progress of computation, manipulation on data
   structures
 warning_
    messages which are reported by mvpa if something goes a little
    unexpected but not critical

.. _verbose: api/mvpa.misc-module.html#verbose
.. _debug: api/mvpa.misc-module.html#debug
.. _warning: api/mvpa.misc-module.html#warning


Verbose Messages
~~~~~~~~~~~~~~~~

Primarily for a user of PyMVPA to provide information about the
progress of their scripts. Such messages are printed out if their
level specified as the first parameter to verbose_ function call is
less than specified. There are two easy ways to specify verbosity
level:

* command line: you can use optVerbose_ for precrafted command
  line option for to give facility to change it from your script (see
  examples)
* environment variable ``MVPA_VERBOSE``
* code: verbose.level property


Warning Messages
~~~~~~~~~~~~~~~~

Reported by PyMVPA if something goes a little unexpected but not
critical. They are printed just once per occasion, i.e. once per piece
of code where it is called.


Debug Messages
~~~~~~~~~~~~~~

Debug messages are used to track progress of any computation inside
PyMVPA while the code run by python without optimization (i.e. without
``-O`` switch to python). They are specified not by the level but by
some id usually specific for a particular PyMVPA routine. For example
``RFEC`` id causes debugging information about `Recursive Feature
Elimination call`_ to be printed (See `misc module sources`_ for the
list of all ids, or print ``debug.registered`` property).

Analogous to verbosity level there are two easy ways to specify set of
ids to be enabled (reported):

* command line: you can use optDebug_ for precrafted command line
  option to provide it from your script (see examples). If in command
  line if optDebug_ is used, ``-d list`` is given, PyMVPA will print
  out list of known ids.
* environment: variable ``MVPA_DEBUG`` can contain comma-separated
  list of ids.
* code: debug.active property (e.g. ``debug.active = [ 'RFEC', 'CLF' ]``)

Besides printing debug messages, it is also possible to print some
metric. You can define new metrics or select predefined ones (vmem,
asctime, pid). To enable list of metrics you can use
``MVPA_DEBUG_METRICS`` environment variable to list desired metric
names comma-separated.

As it was mentioned earlier, debug messages are printed only in
non-optimized python invocation. That was done to eliminate any
slowdown introduced by such 'debugging' output, which might appear at
some computational bottleneck places in the code. 

.. TODO: Unify loggers behind verbose and debug. imho debug should have
   also way to specify the level for the message so we could provide
   more debugging information if desired.

.. _optVerbose: api/mvpa.misc.cmdline-module.html#optVerbose
.. _optDebug: api/mvpa.misc.cmdline-module.html#optDebug
.. _misc module sources: api/mvpa.misc-pysrc.html
.. _Recursive Feature Elimination call: api/mvpa.algorithms.rfe.RFE-class.html#__call__


Additional Little Helpers
-------------------------

(to be written)

.. put information about IO helpers, external bindings, etc here


FSL Bindings
~~~~~~~~~~~~

(to be written)


Frequently Asked Questions
--------------------------

I feel like I want to contribute something, do you mind?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  Not at all! If you think there is something that is not well explained in
  the documentation, send us an improvement. If you implemented a new algorithm
  using PyMVPA that you want to share, please share. If you have an idea for
  some other improvement (e.g. speed, functionality), but you have no
  time/cannot/do not want to implement it yourself, please post your idea to
  the PyMVPA mailing list.

.. We probably need them once we have more than two users.



License
-------

The PyMVPA package, including all examples, code snippets and attached
documentation is covered by the MIT license.

::

  The MIT License

  Copyright (c) 2006-2007 Michael Hanke
                     2007 Yaroslav Halchenko

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to deal
  in the Software without restriction, including without limitation the rights
  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
  copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
  THE SOFTWARE.





.. The following should only be considered when running rst2latex, but Michael
   doesn't know how to do that. If it would work we would get printed
   references to all external link targets. Otherwise we have nice links in the
   PDF, but when they are printed nobody knows where a link points to
   .. raw:: latex
     \theendnotes
   .. target-notes::
