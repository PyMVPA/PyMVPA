.. -*- mode: rst -*-
  ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ###
  #
  #   See COPYING file distributed along with the PyMVPA package for the
  #   copyright and license terms.
  #
  ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ###

PyMVPA Manual
=============

:Authors:
  Michael Hanke <michael.hanke@gmail.com>;
  Yaroslav O. Halchenko <debian@onerussian.com>
:Contact:  pkg-exppsy-pymvpa@lists.alioth.debian.org
:Homepage: http://pkg-exppsy.alioth.debian.org/pymvpa/
:Revision: 0.0.1

.. Please add yourself to the list of authors if you contribute something
   to this manual.

.. contents::
.. sectnum::



Introduction
------------

PyMVPA stands for *Multivariate Pattern Analysis* in Python_.

.. _Python: http://www.python.org

.. should become an abstract


How to cite PyMVPA
~~~~~~~~~~~~~~~~~~
.. come up with something


Prerequisites
~~~~~~~~~~~~~
.. * Basic Python experiences (if not easy to get with Python tutorials).
.. * For efficiency:
..   - prior experience with NumPy and/or SciPy.
..   - basic understanding of the concepts(s) of machine learning.

If you want to use PyMVPA interactively it is strongly recommend to use
IPython_. If you think: *"Oh no, not another one, I already have to learn about
PyMVPA."* please invest a few minutes to watch `some screencasts`_, so at
least you know what you are missing.

.. _IPython: http://ipython.scipy.org
.. _some screencasts: http://showmedo.com/videos/series?name=CnluURUTV


Obtaining PyMVPA
~~~~~~~~~~~~~~~~

Binary packages
'''''''''''''''

Source code
'''''''''''

Installation
~~~~~~~~~~~~
.. Point to source and binary distribution. Preach idea of free software.
   Step by step guide to install it on difficult systems like Windows.

.. Don't forget to mention that the only reasonable way is to use this piece
   of software (like every other piece) is under Debian! Also mention that
   Ubuntu is no excuse ;-)


Conventions
~~~~~~~~~~~
In all examples the NumPy package is assumed to be imported using the alias `N`
throughout this manual.

  >>> import numpy as N


A Bit of History
~~~~~~~~~~~~~~~~

The roots of PyMVPA date back to early 2005. At that time it was a C++ library
(no Python_ yet) developed by Michael Hanke and Sebastian Kr√ºger with the
purpose to easily apply artificial neural networks to pattern recognition
problems.

During a visit to `Princeton University`_ in spring 2005 Michael Hanke
was introduced to the `MVPA toolbox`_ for `Matlab
<http://buchholz.hs-bremen.de/aes/aes_matlab.gif>`_. This toolbox was
and is mainly developed by `Greg Detre`_ and had several advantages
over a C++ library. Most importantly it was easier to use. While a
user of a C++ library is forced to write a significant amount of
front-end code, users of the MVPA toolbox could simply load their data
and start analyzing it. Besides that the MVPA toolbox offered a number
of algorithms that were not implmented in the C++ library.

.. _Princeton University: http://www.princeton.edu
.. _MVPA toolbox: http://www.csbmb.princeton.edu/mvpa/
.. _Greg Detre: http://www.gregdetre.co.uk

However, writing a Matlab toolbox implies some disadvantages that also
apply to the MVPA toolbox. While users in general benefit from the powers
of Matlab, they are at the same time bound to the goodwill of a commercial
company. That this is indeed a problem becomes obvious when one consideres the
time when the vendor of Matlab was not willing to support the Mac platform.
Therefore even if the MVPA toolbox is `GPL-licensed`_ it cannot fully benefit
from the enourmous advantages of the free-software development model
environment (free as in free speech, not only free beer).

.. _GPL-licensed: http://www.gnu.org/copyleft/gpl.html

Under this impression Michael thought that a successor of the C++ library
should remain truely free-software, remain fully object-oriented (in contrast
to the MVPA toolbox), but should be at least as easy to use and extensible
as the MVPA toolbox.

After evaluating some possibilities Michael decided that `Python`_ is the
most promissing candidate that was fully capable to fulfil the intended
development goal. Python is a very powerful language that magically combines
the possibility to write really fast code and a simplicity that allows to
learn the basic concepts within a few days.

One of the major advantages of Python is the availablity of a huge amount of
so called *modules*. Modules can include extensions written in a hard-core
language like C and therefore allow to incorporate high-performance code
without having to leave the Python environment. Additionally some Python
modules even provide links to other toolkits. For example `RPy`_ allows to use
the full functionality of R_ from inside Python. Even Matlab can be used via
some Python modules (see PyMatlab_ for an example).

.. _RPy: http://rpy.sourceforge.net/
.. _R: http://www.r-project.org
.. _PyMatlab: http://code.google.com/p/pymatlab/

After the decision for Python was made, Michael started development with a
simple k-Nearest-Neighbour classifier and a cross-validation class. Using
the mighty NumPy_ package made it easy to support data of any dimensionality.
Therefore PyMVPA can easily be used with 4d fMRI dataset, but equally well
with EEG data (3d) or even non-neuroimaging datasets.

.. _NumPy: http://numpy.scipy.org/

By September 2007 PyMVPA included support for reading and writing datasets
from and to the `NIfTI format`_, kNN and Support Vector Machine classifiers,
as well as several analysis algorithms (e.g. searchlight and incremental
feature search).

.. _NIfTI format: http://nifti.nimh.nih.gov/

During another visit in Princeton in October 2007 Michael met with `Yaroslav
Halchenko`_ and `Per B. Sederberg`_. That incident and the following
discussions and hacking sessions of Michael and Yaroslav lead to a major
refactoring of the PyMVPA codebase, making it much more flexible/extensible,
faster and easier as it has ever been before.

.. _Yaroslav Halchenko: http://www.onerussian.com/
.. _Per B. Sederberg: http://www.princeton.edu/~persed/


Credits
~~~~~~~

.. Please add some notes when you think that you should give credits to someone
   that enables or motivates you to work on PyMVPA ;-)



Overview
--------

The PyMVPA package consists of three major parts: `Data handling`_,
Classifiers_ and Algorithms_ operating on datasets and classifiers.
In the following sections the basic concept of all three parts will be
described and examples using certain parts of the PyMVPA package will be
given. However, this manual does not describe every bit and piece of the
PyMVPA package. For more information, please have a look at the API
documentation, which is a comprehensive and up-to-date description of the
whole package. More examples and usage patterns extending the ones described
here can be taken from the unit test battery, that is part of the source
distribution (in the `tests/` directory).



Data Handling
-------------

The foundation of PyMVPA's data handling is the Dataset_ class. Basically,
this class stores data samples, sample attributes and dataset attributes, where
sample attributes assign a value to each data sample and dataset attributes are
additional information or functionality that applies to the whole dataset.

.. _Dataset: api/mvpa.datasets.dataset.Dataset-class.html

Most likely the Dataset_ class will not be used directly, but through one
of the derived classes. However, it is perfectly possible to use it directly.
In the simplest case a dataset can be constructed by specifying some
data samples and the corresponding class labels.

  >>> from mvpa.datasets.dataset import Dataset
  >>> data = Dataset(samples=N.random.normal(size=(10,5)), labels=1)
  >>> data
  Dataset / float64 10 x 5, uniq: 1 labels, 10 chunks

The above example creates a dataset with 10 samples and 5 features each. The
values of all features stem from normally distributed random noise. The class
label '1' is assigned to all samples. Instead of a single scalar value `labels`
can also be a sequence with individual labels for each data sample. In this
case the length of this sequence has to match the number of samples.

Interestingly the dataset object tells us about 10 `chunks`. In PyMVPA chunks
are used to group subsets of data samples. However, if no grouping information
is provided all data samples are assumed to be in their own group, hence no
sample grouping is performed.

Both `labels` and `chunks` are so called *sample attributes*. All sample
attributes are stored in sequence-type containers consisting of one value per
sample. These containers can be accessed by properties with the same as the
attribute:

  >>> data.labels
  array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
  >>> data.chunks
  array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

The *data samples* itself are stored as a two-dimensional matrix where each
row vector is a `sample` and each column vector contains the values of a
`feature` across all `samples`. The Dataset_ class provides access to the
samples matrix via the `samples` property.

  >>> data.samples.shape
  (10,5)

The Dataset_ class itself can only deal with 2d sample matrices. However,
PyMVPA provides a very easy way to deal with data where each data sample is
more than a 1d vector: `Data Mapping`_


Data Mapping
~~~~~~~~~~~~
It was already mentioned that the Dataset_ class cannot deal with data samples
that are more than simple vectors. This could be a problem in cases where the
data has a higher dimensionality, e.g. functional brain-imaging data where
each data sample is typically a three-dimensional volume.

One approach to deal with this situation would be to concatenate the whole
volume into a 1d vector. While this would work in certain cases there is
definitely information lost. Especially for brain-imaging data one would most
likely want keep information about neighbourhood and distances between data
sample elements.

In PyMVPA this is done by mappers that transform data samples from their
original *dataspace* into the so-called *features space*. In the above
neuro-imaging example the *dataspace* is three-dimensional and the
*feature space* always refers to the 2d `samples x features` representation
that is required by the Dataset_ class. In the context of mappers the
dataspace is sometimes also refered to as *in-space* while the feature space
is labeled as *out-space*.

The task of a mapper besides transforming samples into 1d vectors is to retain
as much information of the dataspace as possible. Some mappers provide
information about dataspace metrics and feature neighbourhood, but all mappers
are able to do reverse mapping from feature space into the original dataspace.

Usually one does not have to deal with mappers directly. PyMVPA provides some
convenience subclasses of Dataset_ the automatically perform the necessary
mapping operations internally. 

For an introduction into to concept of a dataset with mapping capabilities
we can take a look at the MaskedDataset_ class. This dataset class works
almost exactly like the basic Dataset_ class, except that it provides some
additional method and is more flexible with respect to the format of the sample
data. A masked dataset can be created just like a normal dataset.

.. _MaskedDataset: api/mvpa.datasets.maskeddataset.MaskedDataset-class.html

  >>> from mvpa.datasets.maskeddataset import MaskedDataset
  >>> mdata = MaskedDataset(samples=N.random.normal(size=(5,2,3,4)),
  ...                       labels=[1,2,3,4,5])
  >>> mdata
  Dataset / float64 5 x 24 uniq: 5 labels 5 chunks

However, unlike Dataset_ the MaskedDataset_ class can deal with sample
data arrays with more than two dimensions. More precisely it handles arrays of
any dimensionality. The only assumption that is made is that the first axis
of a sample array separates the sample data points. In the above example we
therefore have 5 samples, where each sample is a 2x3x4 volume.

If we look at the self-description of the created dataset we can see, that it
doesn't tell us about 2x3x4 volumes, but simply 24 features. That is because
internally the sample array is automatically reshaped into the aforementioned
2d matrix representation of the Dataset_ class. However, the information about
the original dataspace is not lost, but kept inside the mapper used by
MaskedDataset_. Two useful methods of MaskedDataset_ make use of the mapper:
`mapForward()` and `mapReverse()`. The former can be used to transform
additional data from dataspace into the feature space and the latter performs
the same in the opposite direction.

  >>> mdata.mapForward(N.arange(24).reshape(2,3,4))
  (24,)
  >>> mdata.mapReverse(N.array([1]*mdata.nfeatures)).shape
  (2, 3, 4)

Especially reverse mapping can be very useful when visualizing classification
results and information maps on the original dataspace.

Another feature of mapped datasets is that valid mapping information is
maintained even when the feature space changes. When running some feature
selection algorithm (see Algorithms_) some features of the original features
set will be removed, but after feature selection one will most likely want
to know where the selected (or removed) features are in the original dataspace.
To make use of the neuro-imaging example again: The most convenient way to
access this kind of information would be a map of the selected features that
can be overlayed over some anatomy image. This is trivial with PyMVPA, because
the mapping is automatically updated upon feature selection.

  >>> mdata.mapReverse(N.arange(1,mdata.nfeatures+1))
  array([[[ 1,  2,  3,  4],
          [ 5,  6,  7,  8],
          [ 9, 10, 11, 12]],
         [[13, 14, 15, 16],
          [17, 18, 19, 20],
          [21, 22, 23, 24]]])
  >>> sdata = mdata.selectFeatures([2,7,9,10,16,18,20,21,23])
  >>> sdata
  Dataset / float64 5 x 9 uniq: 5 labels 5 chunks
  >>> sdata.mapReverse(N.arange(1,sdata.nfeatures+1))
  array([[[0, 0, 1, 0],
          [0, 0, 0, 2],
          [0, 3, 4, 0]],
         [[0, 0, 0, 0],
          [5, 0, 6, 0],
          [7, 8, 0, 9]]])

The above example selects nine features from the set of the 24 orginal
ones, by passing their ids to the `selectFeatures()` method. The method
returns a new dataset only containing the nine selected features. Both datasets
share the sample data (using a NumPy array view). Using `selectFeatures()`
is therefore both memory efficient and relatively fast. All other
information like class labels and chunks are maintained. By calling
`mapReverse()` on the new dataset one can see that the remaining nine features
are precisely mapped back onto their original locations in the data space.


Data Splitting
~~~~~~~~~~~~~~

In many cases some algorithm should not run on a complete dataset, but just
some parts of it. One well-known example is leave-one-out cross-validation,
where a dataset is typically split into a number of training and validation
datasets. A classifier is trained on the training set and it's generalization
performance is tested using the validation set.

It is important to strictly separate training and validation datasets as
otherwise no valid statement can be made whether a classifier really generated
an appropriate model of the training data. If this requirement is violated
the results are usually very good generalization performances. However, they
provide no relevant information as they are based on cheating or peeking and
do not describe signal similarities between training and validation datasets.

With the splitter classes, PyMVPA makes dataset splitting easy. All dataset
splitters in PyMVPA are implemented as Python generators, meaning that when
called with a dataset once, they return one dataset split per iteration and
an appropriate Exception when they are done. This is exactly the same behavior
as of e.g. the Python `xrange()` function.

To perform data splitting for the already mentioned cross-validation, PyMVPA
provides the NFoldSplitter_ class. It implements a method to generate
arbitrary N-M splits, where N is the number of different chunks in a dataset
and M is any non-negative integer smaller than N. Doing a leave-one-out split
of our example dataset looks like this:

.. _NFoldSplitter: api/mvpa.datasets.nfoldsplitter.NFoldSplitter-class.html

  >>> from mvpa.datasets.nfoldsplitter import NFoldSplitter
  >>> splitter = NFoldSplitter(cvtype=1)   # Do N-1
  >>> for wdata, vdata in splitter(data):
          # do something
          pass

where `wdata` is the *working dataset* and `vdata` is the *validation dataset*.
If we have a look a those datasets we can see that the splitter did what we
intended:

  >>> split = [ i for i in splitter(data)][0]
  >>> split
  (Dataset / float64 9 x 5 uniq: 1 labels 9 chunks,
   Dataset / float64 1 x 5 uniq: 1 labels 1 chunks)
  >>> split[0].uniquechunks
  array([1, 2, 3, 4, 5, 6, 7, 8, 9])
  >>> split[1].uniquechunks
  array([0])

In the first split, the working dataset contains nine chunks of the original
dataset and the validation set contains the remaining chunk.

The usage of the splitter, creating a splitter object and calling it with a
dataset, is a very common design pattern in the PyMVPA package. Like splitters
there are many more so called *processing objects*. These classes or objects
are instanciated by passing all relevant parameters to the constructor. 
Processing objects can then be called multiple times with different datasets
to perform their algorithm on the respective dataset. This design applies to
virtually every piece of PyMVPA that is described in the Algorithms_ section,
but also the many other parts.



Classifiers
-----------

.. First generic interface of all classifiers in PyMVPA. Point to the special
   case of multi-class classification and how to deal with it. Finally describe
   features of all available classifiers.


k-Nearest-Neighbour
~~~~~~~~~~~~~~~~~~~


Support Vector Machines
~~~~~~~~~~~~~~~~~~~~~~~


Logistic Regression
~~~~~~~~~~~~~~~~~~~


Algorithms
----------

.. Again general overview first. What is a `SensitivityAnalyzer`, what is the
   difference between a `FeatureSelection` and an `ElementSelector`.
   Finally more detailed note and references for each larger algorithm.

.. Explain the concept of having algorithm object instances that configure with
   parameters upon creation and can be used multiple times by calling them with
   datasets. Also attache explaination of PyMVPA object `states` at that point.

.. Point to the difference of `DataMeasure` and `SensitivityAnalyzer`. The
   former computes some value given a dataset (might be scalar or something
   else), whereas that latter computes a 1d map (sensitivity -> features),
   where each value assigns a quantification of the amount of available
   information in that feature.


Searchlight
~~~~~~~~~~~
.. Mention the fact that it also is a special `SensitivityAnalyzer`


Recursive Feature Elimination
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


Incremental Feature Search
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. What are the practical differences (besides speed) between RFE and IFS?


Statistical Testing
~~~~~~~~~~~~~~~~~~~

.. Point to the problem of an unknown H0 distribution, which is a problem
   for a lot of statistical tests.


Frequently Asked Questions
--------------------------

I feel like I want to contribute something, do you mind?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  Not at all! If you think there is something that is not well explained in
  the documentation, send us an improvement. If you implemented a new algorithm
  using PyMVPA that you want to share, please share. If you have an idea for
  some other improvement (e.g. speed, functionality), but you have no
  time/cannot/do not want to implement it yourselve, please post your idea to
  the PyMVPA mailing list.

.. We probably need them once we have more than two users.



License
-------

The PyMVPA package, including all examples, code snippets and attached
documentation is covered by the MIT license.

::

  The MIT License

  Copyright (c) 2006-2007 Michael Hanke
                     2007 Yaroslav Halchenko

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to deal
  in the Software without restriction, including without limitation the rights
  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
  copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
  THE SOFTWARE.





.. The following should only be considered when running rst2latex, but Michael
   doesn't know how to do that. If it would work we would get printed
   references to all external link targets. Otherwise we have nice links in the
   PDF, but when they are printed nobody knows where a link points to
   .. raw:: latex
     \theendnotes
   .. target-notes::
