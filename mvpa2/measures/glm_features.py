# emacs: -*- mode: python; py-indent-offset: 4; indent-tabs-mode: nil -*-
# vi: set ft=python sts=4 ts=4 sw=4 et:
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
#
#   See COPYING file distributed along with the PyMVPA package for the
#   copyright and license terms.
#
### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##
"""Extract features for MVPA using univariate GLM modeling
"""

__docformat__ = 'restructuredtext'

import numpy as np

import mvpa2.datasets
from mvpa2.base import externals
from mvpa2.base.state import ConditionalAttribute

from mvpa2.measures.base import FeaturewiseMeasure
from mvpa2.datasets.base import Dataset

from mvpa2.misc.fx import double_gamma_hrf

if externals.exists('hrf_estimation'):
    import hrf_estimation as he # pip install -U hrf_estimation
if externals.exists('nipy'):
    from nipy.modalities.fmri import hemodynamic_models as hdm

if __debug__:
    from mvpa2.base import debug

def fsl_design_to_evs(fsf_filename):
    """Load FSL design and spit out description of EVs and other regressors
    used in the original analysis.  Each EV should also carry HRF definition
    so that our model then should be 100% match to the one generated by FSL
    """
    pass # TODO
    """should we carry HRFs or should we have a separate structure describing
    ev -> HRF, w or without derivative?"""

def bunch_to_evs(b):
    evs = dict([(c, {'onsets': o, 'durations': d})
                for c, o, d in zip(b.conditions, b.onsets, b.durations)])
    if hasattr(b, 'regressor_names'):
        regressors = dict([(r, v) for r, v in zip(b.regressor_names, b.regressors)])
    else:
        regressors = None
    assert(not hasattr(b, 'tmod') or not b.get('tmod', None))
    assert(not hasattr(b, 'pmod') or not b.get('pmod', None))
    return evs, regressors


def regroup_conditions(evs, groups):
    """Reassign some EVs into others identified by new group names
    """
    out = evs.copy()
    for g, members in groups.iteritems():
        out[g] = sum([out.pop(m) for m in members], [])
    return out


class TrialsResponseEstimator(FeaturewiseMeasure):
    """Given a list of events -- estimate responses per each trial.

    Specific implementations might also provide estimations of HRFs
    per each feature/group of stimuli.

    Estimators to implement/interface

    - hrf_estimation:  use hrf_estimation library to estimate HRF per each
      voxel while events within condition share the same HRF.

     #1
      For that we would need to iterate hrf_estimation for few iterations
      while choosing EV to optimize, while placing others into "drifts".

      Order should not matter -- we will do
      1st step:  all share HRF
      2- step:
        for each EV place others into 'drifts' while taking HRF for them from
        previous step
     #2
      or may be it would be possible to fake extended HRF which would contain
      blocks of FIR for each "group"?

    - FIR:
      just deduce FIR per each voxel/EV using GLM

      pros: should be a stable HRF estimate
      cons: since different trial subtypes might have differing amplitude
            of responses and even shape -- such a mean FIR might be quite
            suboptimal

    - Turner et al: they actually do not estimate HRF for a voxel per se.

      Every EV/voxel would get its own HRF as the output of their
      feature extraction

      pros: account for HRF variability across trials???
      cons: noisier and more features to characterize each trial

    """

    # TODO: We might well want to train separately to estimate HRFs
    #       while only estimating responses in predict
    is_trained = True

    hrfs = ConditionalAttribute(enabled=False,
        doc="Dataset with estimated per each voxel/group HRF")
    design = ConditionalAttribute(enabled=False,
        doc="Design used for the estimation of HRF/responses")
    designed_data = ConditionalAttribute(enabled=True,
        doc="If v0 was provided for rank_one -- store 'designed' data")
    nuisances = ConditionalAttribute(enabled=False,
        doc="Fits to nuisance variables")

    def __init__(self,
                 evs,
                 tr,
                 ev_group_key=None,
                 estimator='hrf_estimation',
                 nuisance_offset=True,
                 nuisance_sas=None,
                 fir_length=20,
                 hrf_gen=double_gamma_hrf,
                 rank_one_kwargs={},
                 **kwargs):
        """
        Parameters
        ----------
        evs : dict of EVs, list of Events
           dict of explanatory variables definitions
            each value would have
             onsets, (in sec)
             durations, (optional, assume 1 if none) TODO -- not used ATM
#             ?hrf, callable which given time points produces HRF kernel for convolution
        ev_group_key : str, optional
            In case of evs being a list of Events, group them
            based on the ev_group_key to estimate separate HRF for each
            group of events.  By default the same HRF estimated all
            events.
        """
        FeaturewiseMeasure.__init__(self, **kwargs)
        self.evs = evs
        self.tr = tr
        self.estimator = estimator
        self.nuisance_sas = nuisance_sas
        self.nuisance_offset = nuisance_offset
        self.fir_length = fir_length
        self.hrf_gen = hrf_gen
        self.rank_one_kwargs = rank_one_kwargs
        self.ev_group_key = ev_group_key

    def _get_nuisances_ds(self, dataset):
        # prepare nuisances
        if not self.nuisance_sas and not self.nuisance_offset:
            return None
        nuisances, nuisance_names, nuisance_indexes = [], [], []
        for sa in (self.nuisance_sas or []):
            # those are samples x regressors (x whatever?)
            sa_value = dataset.sa[sa].value
            nuisance_names += [ sa ] * sa_value.shape[1] \
                              if sa_value.ndim > 1 else [ sa ]
            nuisance_indexes += range(sa_value.shape[1])
            nuisances += list(
                sa_value[None, :] if sa_value.ndim == 1 else np.swapaxes(sa_value, 0, 1))
        if self.nuisance_offset:
            nuisances.append(np.ones((len(dataset),)))
            nuisance_names.append('offset')
            nuisance_indexes.append(0)
        return Dataset(np.array(nuisances).T,
                       sa=dataset.sa.copy(),
                       fa={'names': nuisance_names,
                           'index': nuisance_indexes})

    def _get_groupped_events(self, evs):
        evs_ = {}
        if self.ev_group_key is None:
            # all EVs fall into the same group
            g_func = lambda e: 'ev'
        else:
            k = self.ev_group_key
            def g_func(e):
                if not k in e:
                    raise ValueError("Event %s must be lacking value for %s"
                                     % (e, k))
                return e[k]
        for e in evs:
            g = g_func(e)
            if not g in evs_:
                evs_[g] = []
            evs_[g].append(e)
        return evs_

    def _get_he_design(self, dataset):
        ntimepoints = len(dataset)
        # for hrf_estimator, we would need to generate the design matrix
        # which we would later expand for FIRs
        tr = self.tr

        if not isinstance(self.evs, dict):
            # must be a list of Events
            evs_groupped = self._get_groupped_events(self.evs)
        else:
            if self.ev_group_key is not None:
                raise ValueError("You have provided dictionary of EVs AND ev_group_key"
                                 " which should not be specified in this case")
            evs_groupped = self.evs

        nevs = sum([len(evs) for evs in evs_groupped.itervalues()])
        ngroups = len(evs_groupped)

        # all available descriptors for the EVs would be used as fa's
        # in design and we initiate those as lists with None's entries
        # which we will populate while processing each even of each group
        # in the sparse design matrix
        fa_keys = list(set(sum([e.keys() for e in evs], [])))
        fas_design = dict((k, [None] * nevs*self.fir_length*ngroups)
                          for k in fa_keys)
        # and the one for every event (thus beta)
        fas_evs = dict((k, []) for k in fa_keys)

        # at first do not care about groups, all events are alike
        # TODO: care about groups
        design_shape = (ntimepoints, nevs*self.fir_length*ngroups)
        design = np.zeros(shape=design_shape, dtype=int)
        groups, event_totalindex = [], 0
        for ig, (g, events) in enumerate(evs_groupped.iteritems()):
            # offset for the group's HRF coefficients within the "full HRF"
            goffset = self.fir_length * ig
            groups.append(g)
            for ie, e in enumerate(events):
                e = e.copy()
                o = e['onset']
                d = e.get('duration', 1)
                # Place events rounding to closest tr
                o_idx = int(round(o/tr))
                e_idx = max(int(round((o+d)/tr)), o_idx+1)
                # place the diagonal with 1s for every fir offset
                # into the design.
                ix, iy = [], []
                # which column to start placing this even in this group place
                iy_o = event_totalindex   *self.fir_length*ngroups + goffset
                for resp_point in np.arange(o_idx, e_idx):
                    ix += list(resp_point + np.arange(self.fir_length))
                    iy += range(iy_o, iy_o + self.fir_length)
                ix, iy = np.asanyarray(ix), np.asanyarray(iy)
                # discard those going beyond the duration
                ix_legit = ix < ntimepoints
                ix_ = ix[ix_legit]
                iy_ = iy[ix_legit]
                design[(ix_, iy_)] = 1
                # store event's attributes into fas while checking
                # that it was not yet occupied, since it should have
                # not
                for k, values in fas_design.iteritems():
                    v = e.get(k)
                    # for non single-point duration there might be
                    # multiple same values
                    for j in np.unique(iy_):
                        assert(values[j] is None)
                        values[j] = v
                event_totalindex += 1
                for k, values in fas_evs.iteritems():
                    values.append(e.get(k))
                # we might want to store the entire dict of event for
                # that occasion so we could populate .fa happen we want
                # store this bloody design
        # Add FIR offset fa
        fas_design['fir_offset'] = range(self.fir_length) * (nevs*ngroups)
        # TODO: it might be more efficient to initiate/assign to
        # sparse matrix from the beginning?
        if externals.exists('scipy'):
            import scipy.sparse as ss
            design = ss.csr_matrix(design)
        # import pydb; pydb.debugger()
        return Dataset(design, fa=fas_design), groups, fas_evs # , sa=dataset.sa.copy())
    # groups, event_indexes

    def _call(self, dataset):
        tr = self.tr
        timex = np.arange(0, float(self.fir_length)*tr, tr)
        canonical = self.hrf_gen(timex)

        nuisances = self._get_nuisances_ds(dataset)

        if self.estimator == 'hrf_estimation':
            import hrf_estimation as he
            designds, groups, fas_evs = self._get_he_design(dataset)
            nevs = len(fas_evs.values()[0])
            ngroups = len(groups)
            kwargs = dict(#alpha=0.,#  rtol=0.1e-6, maxiter=10000,
                          verbose=__debug__ and 'HRF_' in debug.active)
            kwargs.update(self.rank_one_kwargs)
            if 'v0' in kwargs:
                design = designds.samples
                design = design if isinstance(design, np.ndarray) else np.array(design.todense())
                v0 = kwargs['v0']
                self.ca.designed_data = \
                    np.sum(design
                           * np.array(list(canonical) * nevs * ngroups) # HRF
                           * np.repeat(v0, self.fir_length * ngroups) # amplitudes
                           , axis=1)
            # having high baseline would affect estimates even if we add a
            # 'constant' as a nuisance.  Thus we will explicitly demean data
            # prior estimation (if nuisance_offset) and then add those means to
            # 'offset'
            out, data_means = [], []
            for voxel_data in dataset.samples.T:
                if self.nuisance_offset:
                    data_mean = np.mean(voxel_data)
                    data = voxel_data - data_mean
                    data_means.append(data_mean)
                else:
                    data = voxel_data
                out.append(he.rank_one(designds.samples,
                                       data,
                                       size_u=self.fir_length * ngroups,
                                       # replicate canonical across groups
                                       # TODO: per group starting HRF?
                                       u0=np.hstack([canonical] * ngroups),
                                       Z=np.asanyarray(nuisances), **kwargs))

            # And now collect results into single arrays
            out = [np.concatenate(x, axis=1) for x in zip(*out)]
            hrfs, betas = out[:2]

            if nuisances is not None:
                W = out[2]
                # TODO: compose usable/useful .sa.nuisances dataset
                if self.ca.is_enabled('nuisances'):
                    self.ca.nuisances = Dataset(W, sa=nuisances.fa, fa=dataset.fa)
                    if self.nuisance_offset:
                        self.ca.nuisances.samples[np.where(nuisances.fa.names == 'offset')[0], :] += data_means

            if self.ca.is_enabled('design'):
                # TODO -- add all the attribution for each sa, fa
                self.ca.design = designds
        else:
            raise NotImplementedError()

        betas = Dataset(betas, fa=dataset.fa, sa=fas_evs) # sas)

        # compose resultant dataset with HRFs
        if self.ca.is_enabled('hrfs'):
            if self.ev_group_key:
                # place different
                # groups into 'columns' while breaking our 'meta'-HRF into pieces
                dss = []
                for i, g in enumerate(groups):
                    fa = {self.ev_group_key: [g] * dataset.nfeatures}
                    fa.update(dataset.fa)
                    dss.append(Dataset(hrfs[i*self.fir_length:(i+1)*self.fir_length], fa=fa))
                hrfsds = mvpa2.datasets.hstack(dss)
                hrfsds.sa['time_coords'] = timex
            else:
                hrfsds = Dataset(hrfs, sa={'time_coords': timex}, fa=dataset.fa)
            self.ca.hrfs = hrfsds

        return betas
